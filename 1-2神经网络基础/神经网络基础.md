## 神经网络基础

2021-4-24

---

[toc]

### 0. mind

![image-20210425184107650](图片\mind.png)

### 1. 二分分类

#### 1.1 定义

给定一张图片，输出0/1，判断它是不是一只猫，这就是二分分类。

![image-20210425131836033](图片\二分分类.png)

#### 1.2 特征向量，label

一幅图像由多个像素组成，我们将图像转换成一个列向量，称之为**feature vector 特征向量x**，维度**n=64\*64\*3=12288**，而y称为结果**label**

![image-20210425131909269](图片\feature vector.png)

#### 1.3 数据集

我们定义**m**为训练样本数（图片数），**(x^1,y^1)**则为第一个样本，**X为m列，n行的矩阵**，**y则为1行，m列的行向量**

![image-20210425131944972](图片\training example.png)



### 2. logistic 回归

#### 2.1 目的

logistic regression 算法适用于计算二分分类，应用它的目的就是解决二分分类问题。给定一个**x**，我们想要得到这个**y^，y帽是一个概率，在x的情况下，y=1的概率**。

![image-20210425132027517](图片\logistic regression.png)



#### 2.2 参数

函数中有两个参数**w,b**，但是**w^tx+b**是一个一次函数，它并不能保证，**y^**落在0-1之间，因此还需要加一个**sigmoid function**.

![image-20210425131024512](图片\logitic function.png)

**sigmoid function**，是用于约束**y^**，落在0-1之间的函数，下图能看到分析，图像，以及**sigmoid function**的写法

![image-20210425131228066](图片\sigmoid function.png)

#### 2.3 特点

**logistic funtion**中，我们只需判断**w, b**，便能得到一个比较好的**y帽=1**。



### 3. 损失函数

#### 3.1 目的

**Lost function**是用于衡量算法的运行情况，是真实值与预测值的误差分析

![image-20210425190831147](图片\loss function.png)



#### 3.2 函数

我们可以使用最简单的平方差来计算预测值与实际之间的差距，但是这个函数得到的结果在后续的梯度下降法计算中不太友善，因此我们改善了选择了另一个函数。

> 特别要注意的是，损失函数是基于一个样本来讨论的，成本函数才是基于整个数据集来进行计算

![image-20210425191253031](图片\loss(error) function.png)

![image-20210425191338393](图片\loss(right) function.png)





### 4. 成本函数

#### 4.1 目的

成本函数是用来衡量并训练（见梯度下降法）logistic 回归中的两个参数的，是基于整个数据集的。损失函数越小意味着误差越小，通过构造成本函数，只需成本函数尽可能小，就会得到较好的损失函数，较好的回归参数，较好的结果

![image-20210425191939896](图片\loss function.png)



### 5. 梯度下降法

#### 5.1 目的

如何去最小化成本函数？我们给出的方法是**梯度下降法**，这本质上是一种导数求导的方法，当求导为0，即找到了最低点，则是我们想要的最小值。

> 下图中我们也会发现为什么上述的平方差方法不太友好，原因是该函数画出的图形并不是单峰的，则对于最小值的计算会比较麻烦。

![image-20210425192646916](图片\gradient descent algorithm.png)



#### 5.2 计算

计算过程可以从三维分解成二维易于理解，我们先计算**w**，通过导数计算的方法，不断更新**w**，当斜率为正，**w**会不断变小，反之则变大，最终达到最低点。公式中的**α**是学习率，通过它来控制每次迭代的步长。

![image-20210425193324560](图片\gradient descent.png)



#### 5.3 logistic 回归中的梯度下降法

在**logistic**回归中使用梯度下降法，我们仅需要把前面的步骤分析一下，得出logistic回归函数，再得出成本函数，我们的目的就是计算成本函数的最小值

![image-20210425202025698](图片\logistic regression recap.png)

### 6. 向量化

#### 6.1 目的

把数据向量化的目的则是加快计算速度，在大数据集上，倘若使用**for**循环遍历所有样本，它是串行计算的，速度会比较慢，把数据集向量化，通过并行计算的方式，我们会发现计算结果大大地提高了。下图则是向量化，非向量化计算例子的比较。

![image-20210425202832485](图片\vectorization.png)



#### 6.2 logistic 回归中的向量化

> 具体内容留至作业题中记录

![image-20210425203331247](图片\vectorizing logistic regression.png)